Complete AWS EKS Cluster
Configuration in this directory creates EKS cluster with different features shown all-in-one cluster (e.g. Managed Node Groups, Worker Groups, Fargate, Spot instances, AWS Auth enabled).

This example can be used to do smoke test.

See configurations in other examples directories for more specific cases.

Usage
To run this example you need to execute:

$ terraform init
$ terraform plan
$ terraform apply
Note that this example may create resources which cost money. Run terraform destroy when you don't need these resources.

Requirements
Name	Version
terraform	>= 0.13.1
aws	>= 3.22.0
kubernetes	~> 2.0
local	>= 1.4
random	>= 2.1
Providers
Name	Version
aws	>= 3.22.0
random	>= 2.1
Modules
Name	Source	Version
disabled_eks	../..	
disabled_fargate	../../modules/fargate	
disabled_node_groups	../../modules/node_groups	
eks	../..	
vpc	terraform-aws-modules/vpc/aws	~> 3.0
Resources
Name	Type
aws_security_group.all_worker_mgmt	resource
aws_security_group.worker_group_mgmt_one	resource
aws_security_group.worker_group_mgmt_two	resource
random_string.suffix	resource
aws_availability_zones.available	data source
aws_eks_cluster.cluster	data source
aws_eks_cluster_auth.cluster	data source
Inputs
No inputs.

Outputs
Name	Description
cluster_endpoint	Endpoint for EKS control plane.
cluster_security_group_id	Security group ids attached to the cluster control plane.
config_map_aws_auth	A kubernetes configuration to authenticate to this EKS cluster.
kubectl_config	kubectl config as generated by the module.
node_groups	Outputs from node groups
